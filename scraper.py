#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import json
import hashlib
import base64
import shutil
from pathlib import Path

import cloudscraper
from bs4 import BeautifulSoup
import cv2
import requests

# ==== é…ç½® ====
BRIGHTNESS_THRESHOLD = 130
BATCH_SIZE = 100
TEMP_DIR = "temp_download"
LOCAL_DIR = "local_images"

# èµ·å§‹ID
START_ID = 342
# æœ€å¤§è¿ç»­404æ¬¡æ•°ï¼ˆçœŸæ­£çš„ç»“æŸï¼‰
MAX_404_COUNT = 5

# ç›®æ ‡ç§æœ‰ä»“åº“
TARGET_REPO = os.environ.get("TARGET_REPO", "")
GITHUB_TOKEN = os.environ.get("GH_TOKEN", "")
TARGET_BRANCH = "main"

# ç›®æ ‡ä»“åº“ä¸­çš„è·¯å¾„
IMAGES_DIR = "ri"
FOLDERS = ["vd", "vl", "hd", "hl"]

scraper = cloudscraper.create_scraper(
    browser={'browser': 'chrome', 'platform': 'windows', 'mobile': False}
)


# ============ GitHub API ============

def github_get_sha(path: str) -> str | None:
    if not GITHUB_TOKEN or not TARGET_REPO:
        return None
    
    url = f"https://api.github.com/repos/{TARGET_REPO}/contents/{path}"
    headers = {
        "Authorization": f"token {GITHUB_TOKEN}",
        "Accept": "application/vnd.github.v3+json"
    }
    
    try:
        resp = requests.get(url, headers=headers, timeout=30)
        if resp.status_code == 200:
            return resp.json().get("sha")
    except:
        pass
    return None


def github_get_json(path: str) -> tuple:
    if not GITHUB_TOKEN or not TARGET_REPO:
        return None, None
    
    url = f"https://api.github.com/repos/{TARGET_REPO}/contents/{path}"
    headers = {
        "Authorization": f"token {GITHUB_TOKEN}",
        "Accept": "application/vnd.github.v3+json"
    }
    
    try:
        resp = requests.get(url, headers=headers, timeout=30)
        if resp.status_code == 200:
            data = resp.json()
            content = base64.b64decode(data["content"]).decode("utf-8")
            return content, data["sha"]
    except Exception as e:
        print(f"âš ï¸ è·å–JSONå¤±è´¥ {path}: {e}")
    return None, None


def github_upload(path: str, content: bytes, message: str, sha: str = None) -> bool:
    if not GITHUB_TOKEN or not TARGET_REPO:
        return False
    
    url = f"https://api.github.com/repos/{TARGET_REPO}/contents/{path}"
    headers = {
        "Authorization": f"token {GITHUB_TOKEN}",
        "Accept": "application/vnd.github.v3+json"
    }
    
    data = {
        "message": message,
        "content": base64.b64encode(content).decode("utf-8"),
        "branch": TARGET_BRANCH
    }
    if sha:
        data["sha"] = sha
    
    try:
        resp = requests.put(url, headers=headers, json=data, timeout=60)
        return resp.status_code in [200, 201]
    except Exception as e:
        print(f"âŒ ä¸Šä¼ å¤±è´¥ {path}: {e}")
        return False


def get_remote_json(path: str, default=None) -> dict:
    content, _ = github_get_json(path)
    if content:
        try:
            return json.loads(content)
        except:
            pass
    return default if default is not None else {}


def save_remote_json(path: str, data: dict, msg: str) -> bool:
    sha = github_get_sha(path)
    content = json.dumps(data, ensure_ascii=False, indent=2).encode("utf-8")
    return github_upload(path, content, msg, sha)


def batch_upload_to_github(upload_queue: list, hash_registry: dict, 
                           folder_counts: dict, last_id: int) -> bool:
    """æ‰¹é‡ä¸Šä¼ æ‰€æœ‰æ–‡ä»¶åˆ°GitHub"""
    if not upload_queue:
        print("ğŸ“­ æ²¡æœ‰éœ€è¦ä¸Šä¼ çš„æ–‡ä»¶")
        return True
    
    print(f"\n{'='*50}")
    print(f"ğŸ“¤ å¼€å§‹æ‰¹é‡ä¸Šä¼  {len(upload_queue)} ä¸ªæ–‡ä»¶")
    print(f"{'='*50}\n")
    
    success_count = 0
    fail_count = 0
    
    for idx, item in enumerate(upload_queue, 1):
        local_path = item["local_path"]
        remote_path = item["remote_path"]
        file_hash = item["hash"]
        
        print(f"[{idx}/{len(upload_queue)}] {remote_path}", end=" ")
        
        try:
            with open(local_path, "rb") as f:
                content = f.read()
            
            if github_upload(remote_path, content, f"Add {remote_path}"):
                hash_registry[file_hash] = remote_path.replace(f"{IMAGES_DIR}/", "")
                success_count += 1
                print("âœ…")
            else:
                fail_count += 1
                print("âŒ")
                folder = remote_path.split("/")[-2]
                if folder in folder_counts:
                    folder_counts[folder] -= 1
        except Exception as e:
            fail_count += 1
            print(f"âŒ {e}")
    
    print(f"\nğŸ“Š ä¸Šä¼ å®Œæˆ: æˆåŠŸ {success_count}, å¤±è´¥ {fail_count}")
    
    # ä¸Šä¼ å…ƒæ•°æ®
    if success_count > 0:
        print("\nğŸ“ æ›´æ–°å…ƒæ•°æ®...")
        
        if save_remote_json(f"{IMAGES_DIR}/hash_registry.json", hash_registry,
                           f"Update hash_registry (+{success_count})"):
            print("  âœ… hash_registry.json")
        
        if save_remote_json(f"{IMAGES_DIR}/count.json", folder_counts, "Update count"):
            print("  âœ… count.json")
        
        # æ›´æ–°è¿›åº¦
        progress = get_remote_json("progress.json", {"last_id": START_ID - 1})
        progress["last_id"] = last_id
        if save_remote_json("progress.json", progress, f"Update progress to {last_id}"):
            print("  âœ… progress.json")
    
    return fail_count == 0


# ============ å·¥å…·å‡½æ•° ============

def build_url(page_id: int) -> str:
    return f"https://img.hyun.cc/index.php/archives/{page_id}.html"


def get_file_hash(filepath: str) -> str:
    sha256 = hashlib.sha256()
    with open(filepath, "rb") as f:
        for chunk in iter(lambda: f.read(8192), b""):
            sha256.update(chunk)
    return sha256.hexdigest()


def ensure_dir(path: str):
    Path(path).mkdir(parents=True, exist_ok=True)


# ============ å›¾ç‰‡å¤„ç† ============

def scrape_images(url: str) -> tuple:
    """
    çˆ¬å–é¡µé¢ä¸­çš„å›¾ç‰‡é“¾æ¥
    è¿”å›: (images_list, status)
    status: "ok" | "video" | "404" | "error"
    """
    print(f"ğŸŒ çˆ¬å–: {url}")
    
    try:
        resp = scraper.get(url, timeout=30)
        
        # æ£€æŸ¥404
        if resp.status_code == 404:
            return [], "404"
        
        resp.raise_for_status()
        resp.encoding = 'utf-8'
    except requests.exceptions.HTTPError as e:
        if "404" in str(e):
            return [], "404"
        print(f"âŒ è¯·æ±‚å¤±è´¥: {e}")
        return [], "error"
    except Exception as e:
        print(f"âŒ è¯·æ±‚å¤±è´¥: {e}")
        return [], "error"
    
    soup = BeautifulSoup(resp.text, "lxml")
    images = []
    
    for idx, link in enumerate(soup.find_all("a", {"data-fancybox": True}), 1):
        href = link.get("href", "")
        if href.startswith("http"):
            images.append({"url": href, "index": idx})
    
    if not images:
        # æ²¡æœ‰å›¾ç‰‡ï¼Œå¯èƒ½æ˜¯è§†é¢‘é¡µé¢
        print(f"ğŸ¬ æ— å›¾ç‰‡ï¼ˆè§†é¢‘é¡µé¢ï¼‰ï¼Œè·³è¿‡")
        return [], "video"
    
    print(f"ğŸ“· æ‰¾åˆ° {len(images)} å¼ å›¾ç‰‡")
    return images, "ok"


def download_image(url: str, save_path: str) -> bool:
    try:
        resp = scraper.get(url, timeout=60, stream=True)
        resp.raise_for_status()
        with open(save_path, "wb") as f:
            for chunk in resp.iter_content(8192):
                f.write(chunk)
        return True
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
        return False


def convert_to_webp(input_path: str, output_path: str) -> bool:
    try:
        img = cv2.imread(input_path)
        if img is None:
            return False
        cv2.imwrite(output_path, img, [cv2.IMWRITE_WEBP_QUALITY, 85])
        return True
    except:
        return False


def analyze_image(path: str) -> dict | None:
    """åˆ†æå›¾ç‰‡ï¼Œè¿”å›åˆ†ç±»æ–‡ä»¶å¤¹"""
    try:
        img = cv2.imread(path)
        if img is None:
            return None
        
        h, w = img.shape[:2]
        if w < 10 or h < 10:
            return None
        
        orientation = "h" if w >= h else "v"
        
        resized = cv2.resize(img, (100, 100))
        lab = cv2.cvtColor(resized, cv2.COLOR_BGR2LAB)
        avg_l = lab[:, :, 0].mean()
        brightness = "d" if avg_l < BRIGHTNESS_THRESHOLD else "l"
        
        folder = orientation + brightness
        print(f"  ğŸ“ {w}x{h} L={avg_l:.1f} â†’ {folder}")
        
        return {"folder": folder}
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        return None


# ============ æœ¬åœ°å¤„ç† ============

def process_page_local(page_id: int, hash_registry: dict, folder_counts: dict, 
                       upload_queue: list) -> str:
    """
    æœ¬åœ°å¤„ç†å•ä¸ªé¡µé¢
    è¿”å›: "success" | "video" | "404" | "error"
    """
    url = build_url(page_id)
    
    print(f"\n{'='*50}")
    print(f"ğŸ“‚ é¡µé¢ ID: {page_id}")
    print(f"{'='*50}")
    
    ensure_dir(TEMP_DIR)
    
    # çˆ¬å–å›¾ç‰‡
    images, status = scrape_images(url)
    
    if status != "ok":
        return status
    
    new_count = 0
    
    for img in images[:BATCH_SIZE]:
        idx = img["index"]
        temp_path = os.path.join(TEMP_DIR, f"temp_{page_id}_{idx}")
        
        print(f"ğŸ“¥ [{idx}/{len(images)}] ä¸‹è½½ä¸­...")
        
        if not download_image(img["url"], temp_path):
            continue
        
        # æ£€æŸ¥é‡å¤
        file_hash = get_file_hash(temp_path)
        if file_hash in hash_registry:
            print(f"  â­ï¸ è·³è¿‡é‡å¤")
            os.remove(temp_path)
            continue
        
        # åˆ†æå›¾ç‰‡
        info = analyze_image(temp_path)
        if not info:
            os.remove(temp_path)
            continue
        
        # ç¡®å®šç›®æ ‡è·¯å¾„
        target_folder = info["folder"]
        folder_counts[target_folder] += 1
        new_num = folder_counts[target_folder]
        
        # æœ¬åœ°ä¿å­˜
        local_folder = os.path.join(LOCAL_DIR, IMAGES_DIR, target_folder)
        ensure_dir(local_folder)
        local_path = os.path.join(local_folder, f"{new_num}.webp")
        
        if not convert_to_webp(temp_path, local_path):
            os.remove(temp_path)
            folder_counts[target_folder] -= 1
            continue
        os.remove(temp_path)
        
        # æ·»åŠ åˆ°ä¸Šä¼ é˜Ÿåˆ—
        remote_path = f"{IMAGES_DIR}/{target_folder}/{new_num}.webp"
        upload_queue.append({
            "local_path": local_path,
            "remote_path": remote_path,
            "hash": file_hash
        })
        
        hash_registry[file_hash] = f"{target_folder}/{new_num}.webp"
        new_count += 1
        print(f"  ğŸ’¾ {local_path}")
    
    print(f"âœ… é¡µé¢ {page_id} å®Œæˆï¼Œæ–°å¢ {new_count} å¼ ")
    return "success"


# ============ ä¸»å‡½æ•° ============

def main():
    print("ğŸš€ å¼€å§‹è¿è¡Œ\n")
    
    if not GITHUB_TOKEN:
        print("âŒ ç¼ºå°‘ GH_TOKEN")
        return
    if not TARGET_REPO:
        print("âŒ ç¼ºå°‘ TARGET_REPO")
        return
    
    print(f"ğŸ“¦ ç›®æ ‡ä»“åº“: {TARGET_REPO}")
    print(f"ğŸ“ å­˜å‚¨ç›®å½•: /{IMAGES_DIR}/\n")
    
    # è·å–è¿œç¨‹æ•°æ®
    print("ğŸ“¥ è·å–è¿œç¨‹æ•°æ®...")
    progress = get_remote_json("progress.json", {"last_id": START_ID - 1})
    hash_registry = get_remote_json(f"{IMAGES_DIR}/hash_registry.json", {})
    folder_counts = get_remote_json(f"{IMAGES_DIR}/count.json", {})
    
    for f in FOLDERS:
        if f not in folder_counts:
            folder_counts[f] = 0
    
    current_id = progress.get("last_id", START_ID - 1) + 1
    print(f"ğŸ“ ä» ID {current_id} å¼€å§‹\n")
    
    # å‡†å¤‡æœ¬åœ°ç›®å½•
    if os.path.exists(LOCAL_DIR):
        shutil.rmtree(LOCAL_DIR)
    ensure_dir(LOCAL_DIR)
    
    upload_queue = []
    last_success_id = current_id - 1
    consecutive_404 = 0
    
    # ========== é˜¶æ®µ1: æœ¬åœ°å¤„ç† ==========
    print("=" * 60)
    print("ğŸ“¥ é˜¶æ®µ1: æœ¬åœ°ä¸‹è½½å’Œå¤„ç†")
    print("=" * 60)
    
    while True:
        result = process_page_local(
            current_id, 
            hash_registry, 
            folder_counts, 
            upload_queue
        )
        
        if result == "success":
            last_success_id = current_id
            consecutive_404 = 0
            current_id += 1
            
        elif result == "video":
            # è§†é¢‘é¡µé¢ï¼Œè·³è¿‡ç»§ç»­
            last_success_id = current_id  # ä¹Ÿç®—å¤„ç†è¿‡äº†
            consecutive_404 = 0
            current_id += 1
            
        elif result == "404":
            consecutive_404 += 1
            print(f"âš ï¸ 404 (è¿ç»­: {consecutive_404}/{MAX_404_COUNT})")
            
            if consecutive_404 >= MAX_404_COUNT:
                print(f"\nâ¹ï¸ è¿ç»­ {MAX_404_COUNT} ä¸ª404ï¼Œåˆ°è¾¾æœ«å°¾")
                break
            
            current_id += 1
            
        else:
            # å‡ºé”™
            print(f"\nâŒ å¤„ç†å‡ºé”™ï¼Œåœæ­¢")
            break
    
    # æ¸…ç†ä¸´æ—¶ç›®å½•
    if os.path.exists(TEMP_DIR):
        shutil.rmtree(TEMP_DIR)
    
    # ========== é˜¶æ®µ2: æ‰¹é‡ä¸Šä¼  ==========
    print("\n" + "=" * 60)
    print("ğŸ“¤ é˜¶æ®µ2: æ‰¹é‡ä¸Šä¼ åˆ° GitHub")
    print("=" * 60)
    
    if upload_queue:
        print(f"\nğŸ“Š å¾…ä¸Šä¼ : {len(upload_queue)} ä¸ªæ–‡ä»¶")
        for f in FOLDERS:
            count = sum(1 for item in upload_queue if f"/{f}/" in item["remote_path"])
            if count > 0:
                print(f"   {f}: {count} å¼ ")
        
        batch_upload_to_github(
            upload_queue, 
            hash_registry, 
            folder_counts, 
            last_success_id
        )
    else:
        print("\nğŸ“­ æ²¡æœ‰æ–°å›¾ç‰‡")
        # ä»ç„¶æ›´æ–°è¿›åº¦
        progress["last_id"] = last_success_id
        save_remote_json("progress.json", progress, f"Update progress to {last_success_id}")
    
    # æ¸…ç†
    if os.path.exists(LOCAL_DIR):
        shutil.rmtree(LOCAL_DIR)
    
    print("\nğŸ å®Œæˆ")


if __name__ == "__main__":
    main()
